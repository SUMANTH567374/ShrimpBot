

import torch
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline
from langchain_huggingface import HuggingFacePipeline
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser

# === Configuration ===
MODEL_NAME = "google/flan-t5-base"
MAX_CONTEXT_CHARS = 1000
MAX_TOOL_CHARS = 600

# === Load FLAN-T5 model and tokenizer ===
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
model = AutoModelForSeq2SeqLM.from_pretrained(
    MODEL_NAME,
    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
    device_map="auto" if torch.cuda.is_available() else None
)

# === Create HuggingFace text2text-generation pipeline ===
text_gen_pipeline = pipeline(
    "text2text-generation",
    model=model,
    tokenizer=tokenizer,
    max_new_tokens=256,
    temperature=0.7,
    top_k=40,
    top_p=0.9,
    do_sample=True,
)

# === LangChain LLM wrapper ===
llm = HuggingFacePipeline(pipeline=text_gen_pipeline)

# === Load prompt templates ===
with open("prompts/retrieval.txt", "r", encoding="utf-8") as f:
    retrieval_prompt = PromptTemplate.from_template(f.read())

with open("prompts/synthesis.txt", "r", encoding="utf-8") as f:
    synthesis_prompt = PromptTemplate.from_template(f.read())

# === Output parser to convert generation to string ===
parser = StrOutputParser()

# === LangChain chains for synthesis and retrieval ===
synthesis_chain = synthesis_prompt | llm | parser
retrieval_chain = retrieval_prompt | llm | parser

def run_llm(context: str, tools: str, question: str, refine_question: bool = False) -> str:
    """
    Run the FLAN-T5 model to synthesize an answer to a user's question.

    Args:
        context (str): Retrieved context from SOP documents.
        tools (str): Output generated by domain-specific tools.
        question (str): The original user question.
        refine_question (bool, optional): Whether to rephrase the question using a retrieval chain. Default is False.

    Returns:
        str: Synthesized answer or fallback message.
    """
    try:
        #  refine the question
        if refine_question:
            question = retrieval_chain.invoke({"question": question.strip()})

        # Prepare inputs
        inputs = {
            "context": context.strip()[:MAX_CONTEXT_CHARS],
            "tools": tools.strip()[:MAX_TOOL_CHARS],
            "question": question.strip()
        }

        # Get synthesis output
        response = synthesis_chain.invoke(inputs)

        #Extract final answer if formatted
        if "Final Answer:" in response:
            return response.split("Final Answer:")[-1].strip()

        return response or "⚠️ I couldn't find a clear answer."

    except Exception as e:
        return f"⚠️ FLAN error: {str(e)}"
